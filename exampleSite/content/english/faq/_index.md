---
title: "FAQ"
date: 2021-07-06T15:27:17+06:00
draft: false
# page title background image
bg_image: "images/backgrounds/page-title.jpg"
# about image
image: "images/about/about-page.jpg"
# meta description
description : "FAQ"
# taxonomy
category: "Mathematics"
# teacher
teacher: "Jacke Mastio"
# duration
duration: "06 Month"
# weekly
weekly: "03 hours"
# course fee
fee: "From: $699"
# apply url
apply_url: "#"
# download link
download_link: "#"
# type
# type: "product" # "course"  # "research" # "event" # "notice" # "product" # "training"
---

***

#### SQL Server Big Data Cluster

Data lake
A SQL Server big data cluster includes a scalable HDFS storage pool.
This can be used to store big data, potentially ingested from multiple external sources.
Once the big data is stored in HDFS in the big data cluster,
you can analyze and query the data and combine it with your relational data.

Scale-out data mart
SQL Server Big Data Clusters provide scale-out compute and storage to improve the performance
of analyzing any data. Data from a variety of sources can be ingested and distributed
across data pool nodes as a cache for further analysis.

Integrated AI and Machine Learning
SQL Server Big Data Clusters enable AI and machine learning tasks on the data
stored in HDFS storage pools and the data pools.
You can use Spark as well as built-in AI tools in SQL Server, using R, Python, Scala, or Java.

***

#### …………………… UNFINISHED TEXT

Four different types of analytics – descriptive, diagnostic, 
predictive and prescriptive – ordered by the level of difficulty 
and business value. 

Prescriptive analytics, as the most complex level but offering 
the greatest value, is at the top of that escalator. 

Prescriptive analytics gives a recipe for business success in form 
of actions by answering the key question: “How can we make it happen?”.

https://www.worldprogramming.com/blog/datascience/credit_scoring_pt8/

***

#### I want to use your models as a service - but does that mean I need to open my data?

…………………… UNFINISHED TEXT

…………………… UNFINISHED TEXT

***

#### I have data and need more precise predictive models - why should I work with Qauzativ?

…………………… UNFINISHED TEXT

…………………… UNFINISHED TEXT

***

#### What is the technology stack in DataStore, DataFactory?

We deliver our promise with battle-proven stack components:

* We use SQL Server for the open source and SaaS versions

* Database schema that converts credit reports into
  your own mini-copy of the credit histories database

***

#### How is `Core machine learning track` different from online courses?

Our training programs are supplemented by online courses.

The deliverables:

* `DataCamp` short interactive online courses

* R & Python code libraries with end-to-end solutions

* Lectures by PhDs and university professors

* Instructor-led labs by DS/ML practitioners

* Practice-based projects with credit risk modeling being the base learner

* Kaggle real-life cases as additional projects

***

#### Model interpretability

Accept black boxes no more, regardless of the algorithm, for mission-critical
data driven decisions where the cost of error is too high.
Model interpretability opens doors to using more sophisticated and more
accurate algorithms instead of 'good old' logistic regression and decision trees.
Avoiding bad credit decisions saves a lot of money.

We use [SHAP](https://github.com/slundberg/shap/)
and [LIME](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)
model interpretation methods.

***

#### DataCamp

[DataCamp](https://www.datacamp.com) is the world's leading provider
of practice-based online courses that are developed by 160+ industry experts
and academia who have have contributed to a library of more than 300 courses
on R, Python, SQL, Git, Shell, and other programming languages and platforms.

Trainees get DataCamp subscriptions up to 1-year to continue their education
beyond the syllabus by working through 300+ courses.
There is ample choice of courses in data visualization, quantitative finance
and risk management by industry experts.

DataCamp has more than 3.3 million users in over 190 countries and 1,000+
business customers, the latter of which include EA, Lego, Airbnb, Forrester,
Whole Foods, PayPal, T-Mobile, Telefonica, Kaiser Permanente, HSBC, REI, eBay,
Uber, Ikea, and Mercedes-Benz.
[Source](https://venturebeat.com/2018/12/17/datacamp-raises-25-million-for-customizable-online-data-science-courses/)

The `DataCamp for Business` portal is provided to its enterprise customers
that allows them to track trainees' progress over time.
Team managers can assign particular courses or chapters, see who’s meeting
deadlines, and create custom curricular tracks.

***

#### Data analytics comes as a by-product

With the skills that trainees will learn in the `Core machine learning course`,
they can perform BI analysis and visualization in RStudio IDE, Power BI,
SAS / SPSS and other software that runs R code.

***

#### What is inside your code libraries?

The R / Python code libraries cover the entire 
[CRISP-DM cycle](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)
workflow inside which we pay special attention to areas that are covered
lightly by textbooks and online courses but take hours to build code in real life,
such as treatment of outliers, anomalies & missing values, variable discretization,
variable importance and feature selection in general.

The code comes in 2 forms: inside RMarkdown files where code is executable,
and as a html/pdf book that can be used as a manual/textbook for training
new employees or for refreshing the skills.

SQL and XSLT code comes with both `Datastore` editions.
Please visit those products' pages for more information.

***

#### Our employees can take online courses on their own. Why take Core ML track?

Yes, it is certainly a possibility. Some of your employees most likely have completed
quite a number of online courses and may have degrees and certificates of achievement.

The thing is that skill tracks like 'Data Scientist' and 'Data Analyst'
by Udacity, DataCamp, DataQuest and universities take 6-8 months of persistent learning
@10-20 hours per week, i.e. 300-450 hours of consistent practice.
A few links to data science online programs are listed in the *Useful links* section of
the [Core machine learning track](http://127.0.0.1:4321/training/core-machine-learning-track/) page.

How many of your full-time employees have enough time, motivation and perseverance
on weekday evenings and weekends to invest in it and actually finish those courses?
The honest answer is -- not too many.
Full-time work doesn't combine too nicely with full-loaded studying.

Our `Core machine learning track` covers approximately the same amount of topics as in many
'Data scientist' online programs and comes not only with ready-to-use code libraries
but also with guided training and real-life tasks.

On a more general note, the world is full of DS/ML books, online courses and tutorials
with which one can acquire data science skills relatively quickly.
There is one problem with those resources, though.
They are quite generic.

Albeit absolutely necessary for learning DS/ML, books, courses and tutorials
do not render themselves too nicely to 'copy & paste' in DS/ML tasks.
Online courses are probably the most effective and popular way to learn DS/ML,
especially when the learning is enhanced by good textbooks.
However textbooks and courses often barely cover certain topics that are very
important in real life: feature selection, model selection, treatment of
outliers, anomalies and missing values, variable discretization,
bias-variance tradeoff and the latest "big thing", model interpretability.
There are reasons for that: high quality online courses have extensive
curricula that span 6-8 months and pace students through the topics.
Course authors do not give away full sets of ready-to use code for solving
those vitally important tasks because students are supposed for write their own
code in the process of learning.

Tutorials are an effective quick & dirty method to get around some problems.
But only some problems, and tutorials are quite far from being end-to-end solutions.

People spend hundreds of hours to gain hands-on understanding of those
topics by gathering knowledge from multiple resources like textbooks,
online courses and tutorials in order to write code to solve tasks
that are most often repetitive, i.e. they re-occur in each and every
project and relate mostly to data exploration, feature selection, etc.
Multiply 100 or 200 hours by the number of junior data scientists
-- that's how much payroll money could be spent on solving these under-taught
tasks alone, not to mention recurring tasks in data exploration.

***

#### We can hire data scientists. Why train more people?

True, that is another possibility. There are a few problems with that, however:

* There is a global lack of skilled data science professionals
* Such professionals are expensive
* They tend to seek cutting edge projects that use the latest technologies
* It often takes 3-4 months for fresh graduates to start shipping high quality code
  and models that deliver monetary value to the business
* Having data scientists on board doesn't guarantee that their work will be replicated
  and scaled across the entire DS/ML team

Also the following is worth mentioning:

* It is easier and faster to teach DS/ML to 10 business experts than to train
  3-5 data scientists in 10 different business domains
* It is also cheaper. Those 10 business experts are likely your employees, hence
  no extra cost for expanding the DS/ML team
* Business domain experts will become valuable data analysts upon learning DS/ML skills

***

#### What if we do not have too many people for DS/ML training?

Then our cloud SaaS `DataFactory Azure` offering may be the right match for
your requirements so that your organization gets started with ML powered applications
without making upfront investments in DS/ML.

If you have 2-3 prospective trainees for our
[learning tracks](http://127.0.0.1:4321/training/training-overview/),
then contact us to join an upcoming training session.

***

#### How do your products and services relate to big data and AI?

Our products and services are related but not exactly about big data and AI.

Big data is more about data engineering and technology stack rather than about ML per se.
Big data stack typically includes Apache Spark, Apache Kafka and other applications
that collect, process and store high velocity, high-volume, high-variety data.
There is no contradiction between our products and big data, of course.
The standard tools of machine learning and deep learning are applicable
to big unstructured data and structured relational databases alike.
`DataStore` can be rolled out on SQL Server 2019 edition which is big-data ready with
built-in support for Apache Spark and HDFS.

AI is more of a marketing than a technical term.
Technically AI is based on deep neural network architectures for which we offer
[`Deep learning fundamentals`](http://127.0.0.1:4321/training/deep-learning-fundamentals/).

***

#### What about programming languages -- R, Python, Julia, SAS / SPSS?

##### We deliver code in both languages -- R + Python

In our view, it is not R vs Python. It is R + Python. Python has its strengths and
weaknesses and so does R.
R language gets
[somewhat better support](https://en.wikipedia.org/wiki/R_(programming_language)#Commercial_support_for_R)
than Python in analytics software like SAS, SPSS, Tableau, Power BI, etc.
There is no longer a dilemma of R/Python versus SAS/SPSS. Use both options.
Run R code along with or instead of native SAS/SPSS code.
For companies that already use SAS or SPSS, R may very well be a better choice than Python
because R’s "Foreign" package works with SAS and SPSS data files and SAS has an R interface.
R code runs in SAS and SPSS which means zero extra costs for using R language
while preserving existing codebase in SAS / SPSS.

_**Time has never been better to slash costs on DS/ML by using open source software**_

##### _R language_

R has a lot of financial packages written by people in the field, and many specialist
packages that either don’t exist, or are far less mature, in R’s closest rival, Python.
Like for finance, R has a lot of demographics packages and resources that make it easy
to use domain knowledge within the R ecosystem. Also R is the dominant language in
bioinformatics research where Python has little to offer.
The [BioConductor project](https://www.bioconductor.org/) is a free, open source
and open development software project that is based on R language and provides
tools for the analysis and comprehension of high-throughput genomic data.
A wealth of methods from bioinformatics are available only in R.

R has excellent visualization capabilities --
[Plotly](https://plot.ly/r/dashboard/),
[flexdashboards](https://rmarkdown.rstudio.com/flexdashboard/examples.html),
interactive [Shiny apps](https://shiny.rstudio.com/gallery/)
and [RStudio powered graphics](http://www.ggplot2-exts.org/gallery/)
-- are striking data visualization tools for delivering data analytics.

If required, commercial support is available from
[RStudio Inc.](https://www.rstudio.com/) for RStudio Server, IDE and other products.
If the public R distribution is not enough, Microsoft Corporation provides
Microsoft R Open, the enhanced distribution of R, also free and open source,
and proprietary R packages that greatly enhance performance for RAM and CPU intensive
operations on servers and desktops alike.

##### The IDE

Full-fledged and feature-rich, the RStudio IDE provides better experience
than Python environments like Jupyter Notebooks, PyCharm or Spyder,
especially for people with no prior coding background who are not used to
command line interface and minimalistic environments such as Visual Studio Code,
Atom or Sublime text editors.
RStudio makes R feel like a unified, complete ecosystem.

Better yet, RStudio IDE runs Python, C++ and SQL code, creating a smooth multi-language
working experience for data scientists where Python and R objects & code
can be used interchangeably.

##### Possible future

Speaking of medium to long-term prospects, our view is that neither R nor Python will be
the dominant language of data science in 3 to 5 years. The reason is that both are slow.
Data science is getting ever more computationally intensive to the point where R & Python
can no longer deliver acceptable performance, especially with massive matrix calculations
in deep neural network architectures.

Although one can build deep neural networks using R and Python with a variety
of deep learning frameworks: Tensorflow, Apache mxNet, Microsoft CNTK, Keras,
[**Julia language**](https://julialang.org/) will likely emerge as the leading language
of mathematics and DS/ML. It combines the speed of C++ with the high level expressiveness
of R & Python and mathematical notation of Matlab.

***

#### We already have SAS, SPSS or similar analytics software. Why bother about Python and R?

Expensive proprietary analytics software with graphical user interface can be
easy to learn but costly to scale. Manipulating data with mouse clicks may be
easy to do but difficult to scale up due to high license costs and the time
required for new employees to learn that software.
Data wrangling via mouse clicking also doesn't render itself very well to
technical documentation, as opposed to programming code.

`TeamHub` can help shift focus from relying on the graphical user interface
to using standardized code libraries that come with code comments, web links,
text and graphs, like in a textbook. Except that unlike a textbook,
our online book, powered by `TeamHub`, has executable code that can be launched
by any user, so that emphasis is put more on processes & people rather than software.

***

#### What is CDM, or Common Data Model?

The Common Data Model (CDM) is a standard and extensible collection of schemas
(entities, attributes, relationships) that represents business concepts
and activities with well-defined semantics, to facilitate data interoperability
and provide a standard, structured view of common business data elements.
Examples of entities include: Account, Contact, Lead, Opportunity, Product, etc.

There are several `Dynamics 365` apps whose data is stored in CDM:
Sales, Field Service, Customer Service, Project Service Automation,
Marketing, Financials, and Operations.
Each app ringfences an area of CRM or ERP functionality, and can be used
in conjunction with a wider package of apps, or on its own.
Data from your ERP apps can be stored in Azure Data Lake according
to the CDM schema with minor or no adjustments.

'Finance and Operations': accounts payable, accounts receivable, budgeting,
cash and bank management, asset management, and general ledger.

'Sales': sales, customer service, marketing, digital commerce, and field service.

'Retail': discounts, promotions, loyalty programs, inventory reports,
employee schedules, customer order histories.

'Marketing': multi-channel campaigns and events, personalized,
targeted email campaigns, social engagement.

A few useful links:

[CDM and Azure Data Lake](https://docs.microsoft.com/en-us/common-data-model/data-lake)

[What users need to know about CDM](https://www.zdnet.com/article/microsofts-common-data-model-what-users-not-just-dynamics-365-ones-need-to-know/)

***

#### What is the value of Azure apps and services?

[PowerApps](https://docs.microsoft.com/en-us/powerapps/)
is a suite of apps, services, connectors and data platforms that provides a rapid
application development environment to build custom apps for your business needs.
Using PowerApps, you can quickly build custom business apps that connect to your
business data stored either in the underlying data platform (Common Data Service)
or in various online and on-premises data sources (SharePoint, Excel, Office 365,
Dynamics 365, SQL Server, and so on).

The core concept of PowerApps is that users can connect to business databases
and applications, build a UI to manage the data via drag-and-drop, and then publish
it out to anyone that want to use it.

Apps built using PowerApps provide rich business logic and workflow capabilities
to transform your manual business processes to digital, automated processes.
Further, apps built using PowerApps have a responsive design, and can run seamlessly
in browser or on mobile devices (phone or tablet).
PowerApps "democratizes" the custom business app building experience by enabling
users to build feature-rich, custom business apps without writing code.

The core concept of PowerApps is that end users can connect to business databases
and applications, build a UI to manage the data via drag-and-drop, and then publish
it out to anyone that want to use it.

[ML pipelines in Azure Machine Learning Service](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines)

[Pipelines and activities in Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities)

[Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/)

[Create a data factory by using the Azure Data Factory UI](https://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal)

[Introduction to Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/introduction)

***

#### Useful links: datawarehouse design

Datawarehouses, data lakes and datamarts are a big topic revolving around
two major datawarehouse philosophies, Inmon and Kimball.
Our design of `DataStore` is in line with Bill Inmon's paradigm.
Please see below a few links on DWH design.

While the source data in `DataStore` is structured into many tables
linked with each other through foreign keys to ensure data integrity
and minimize data redundancy, denormalized datamarts can be created as and when
required. In practice 'denormalized' usually means that tables with 'final'
data such as variables for predictive analytics and modeling will be created
for use in BI, reporting, DS/ML and other projects.
Strict 3NF data atomicity requirements do not apply to denormalized datamarts.

* [Inmon or Kimball: Which approach is suitable for your data warehouse?](https://www.computerweekly.com/tip/Inmon-or-Kimball-Which-approach-is-suitable-for-your-data-warehouse)
* [Data Warehouse Design – Inmon versus Kimball](http://tdan.com/data-warehouse-design-inmon-versus-kimball/20300)
* [Inmon vs. Kimball - The Big Data Warehouse Duel](https://xplenty-blog.herokuapp.com/blog/inmon-vs-kimball-the-big-data-warehouse-duel)
* [Kimball and Inmon DW Models](https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/)
* [Quora -- What is normalized vs. denormalized data?](https://www.quora.com/What-is-normalized-vs-denormalized-data)
* [Quora -- Is data warehouse normalized or denormalized? Why?](https://www.quora.com/Is-data-warehouse-normalized-or-denormalized-Why)

***

#### Useful links: ML in Azure and SQL Server

* [SQL Server technologies](https://docs.microsoft.com/en-us/sql/sql-server/sql-server-technical-documentation?toc=..%2ftoc%2ftoc.json&view=sql-server-2017)
* [Apache Spark and HDFS in SQL Server 2019](https://cloudblogs.microsoft.com/sqlserver/2018/09/25/introducing-microsoft-sql-server-2019-big-data-clusters/)
* [Big data capabilities of SQL Server 2019](https://cloudblogs.microsoft.com/sqlserver/2018/09/24/sql-server-2019-preview-combines-sql-server-and-apache-spark-to-create-a-unified-data-platform/)
* [Graph database in SQL Server 2017](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-architecture?view=sql-server-2017)
* [Microsoft Machine Learning R Templates with SQL Server](https://github.com/microsoft/SQL-Server-R-Services-Samples)
* [Microsoft Machine Learning Server Samples and Solutions](https://github.com/Microsoft/ML-Server)
* [SQL Server editions and support of R / Python](https://www.microsoft.com/en-us/sql-server/sql-server-2017-editions)

> In case you haven't found the answer to your question please feel free to contact us.
